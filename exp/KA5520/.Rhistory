}
vectores = function(c1,c2,c3)
{
cat(act.17(c1)"\n",act.17(c2)"\n",act.17(c3)"\n")
}
vectores = function(c1,c2,c3)
{
cat(act.17(c1)\n,act.17(c2)\n,act.17(c3)\n)
}
vectores = function(c1,c2,c3)
{
cat(act.17(c1),act.17(c2),act.17(c3))
}
vectores(c1,c2,c3)
shapiro.test(c1)
vectores(a,c2,c3)
vectores = function(c1,c2,c3)
{
cat(act.17(c1),act.17(c2),act.17(c3),'\n')
}
vectores(a,c2,c3,)
vectores(a,c2,c3)
vectores = function(c1,c2,c3)
{
cat(act.17(c1),'\n',act.17(c2),'\n',act.17(c3),'\n')
}
vectores(a,c2,c3)
predict(m, future)
library(prophet)
df <- read.csv('https://github.com/gumdropsteve/datasets/raw/master/views.csv')
names(df) <- c('ds', 'y')
m <- prophet(df)
future <- make_future_dataframe(m, periods=365)
predict(m, future)
m <- prophet(df)
library(prophet)
df <- read.csv('https://github.com/gumdropsteve/datasets/raw/master/views.csv')
names(df) <- c('ds', 'y')
m <- prophet(df)
future <- make_future_dataframe(m, periods=365)
predict(m, future)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
library(Prophet)
install.packages('prophet')
library(prophet)
df <- read.csv('https://github.com/gumdropsteve/datasets/raw/master/views.csv')
names(df) <- c('ds', 'y')
m <- prophet(df)
m <- fbprophet(df)
library(fbprophet)
install.packages('fbprophet')
t<- c(14,25)
no.t <- c(159,152)
datos <- rbind(t,no.t)
datos
colnames(datos, "F", "M")
datos
datos <- colnames("F", "M")
colnames(datos) <- c("F","M")
datos
sum.filas <- apply(datos,1,sum)
sum.filas
datos/sum.filas
table(datos)
datos
table(datos)
sum.cols <- apply(datos,2,sum)
sum.cols <- apply(datos,2,sum)
rel.filas <- datos/sum.filas
rel.cols <- datos/sum.cols
rel.filas
rel.cols
mosaicplot(rel.filas,rel.cols)
mosaicplot(datos)
mosaicplot(datos)
mosaicplot?
x <- c(1,2,3,4,5,6,7,8) # devuelve los enteros de 1 a 6
chisq.test(datos)
test_1 <- chisq.test(datos)
test_1
test_1$observed
summary(test_1)
test_1$expected
n <- c(20,18,12,17,67)
s <- c(6,22,15,13,56)
e <- c(4,6,14,11,35,)
o <- c(10,19,23,40,92)
n <- c(20,18,12,17,67)
s <- c(6,22,15,13,56)
e <- c(4,6,14,11,35)
o <- c(10,19,23,40,92)
rbind(n,s,e,)
rbind(n,s,e,o)
datos2 <- rbind(n,s,e,o)
datos2
colnames(datos2)<-c("A","B","C","D","E")
datos2
sum.filas2 <- apply(datos2,1,sum)
sum.cols2 <- apply(datos2,2,sum)
sum.filas2
sum.cols2
anova.datos2 = aov(sum.filas2 ~ sum.cols2, data=datos2)
test_2 <- chisq.test(datos2)
test_2
test_2 <- chisq.test(datos2)
qchisq(0.99,datos2)
qchisq(0.99,test_2)
qchisq(0.99,datos2)
qchisq(0.99,12)
fisher.test(datos2)
prop.table(datos2)
e.n <- c(37, 1334)
e.p <- c(11,223)
datos3 <- rbind(e.n,e.p)
datos3
colnames(datos3) <- c("c.a","s.a")
datos3
prop.table(datos3)
prop.table(datos3)*100
round(prop.table(datos3)*100,2)
test_3 <- chisq.test(datos3)
test_3
sin.cint <- c(13827,2642, 1871, 4840, 23180)
con.cint <- c(1229, 622, 1287, 4581, 7719)
sin.cint
rbind(sin.cint,con.cint)
tabla1 <- rbind(sin.cint,con.cint)
tabla1
colnames(tabla1) <- c("grave", "menor", "minima", "no")
sin.cint <- c(13827,2642, 1871, 4840)
con.cint <- c(1229, 622, 1287, 4581)
tabla1 <- rbind(sin.cint,con.cint)
colnames(tabla1) <- c("grave", "menor", "minima", "no")
chisq.test(tabla1)
tabla2 <- rbind(borracho,normal)
borracho <- c(14691, 648, 1164, 4444)
tabla2 <- rbind(borracho,normal)
borracho <- c(14691, 648, 1164, 4444)
normal <- c(365, 2616, 1994, 4977)
tabla2 <- rbind(borracho,normal)
colnames(tabla2) <- c("grave", "menor", "minima", "no")
chisq.test(tabla2)
test1 <- chisq.test(tabla1)
test2 <- chisq.test(tabla2)
test1$residuals
test2$residuals
library(corrplot)
corrplot(test1$residuals,is.corr=FALSE)
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para gráficos
library(plotrix) ##Paquete para gráficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimización del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
S1=c(3.3,4.4,4.9,4.9,3.9,4.2,4.7,5.1,4.6,4.5)
S2=c(4.6,4.5,5.0,4.0,4.5,5.2,4.9,5.5,4.8,5.3)
S3=c(6.7,5.8,5.0,4.8,5.3,6.2,5.0,6.4,5.9,5.4)
S4=c(6.3,6.0,6.7,5.5,6.6,6.1,5.3,6.5,6.3,6.8)
supl=cbind(S1,S2,S3,S4)
efic=data.frame("Suplemento"=factor(c(rep(1,10),rep(2,10),rep(3,10),rep(4,10))),
"Eficiencia"=c(S1,S2,S3,S4))
efic
install.packages("ggrepel")
install.packages("plotrix")
install.packages("smacof")
install.packages("UsingR")
install.packages("pgirmess")
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para gráficos
library(plotrix) ##Paquete para gráficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimización del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
medias=apply(sup1,2,mean)
supl=cbind(S1,S2,S3,S4)
efic=data.frame("Suplemento"=factor(c(rep(1,10),rep(2,10),rep(3,10),rep(4,10))),
"Eficiencia"=c(S1,S2,S3,S4))
efic
medias=apply(sup1,2,mean)
medias=apply(supl,2,mean)
desvios=apply(supl,2,sd)
Resumen3=round(cbind(medias,desvios),3)
Resumen3
install.packages("MASS")
install.packages("car")
library(readxl) ##Permite leer archivos xlsx
library(ggplot2) ##Paquete para confeccionar dibujos
library(ggrepel) ##Paquete que manipula etiquetas para gráficos
library(plotrix) ##Paquete para gráficos requerido para la libreria smacof
library(smacof)  ##Paquete para MDS basado en la minimización del stress
library(UsingR)
library(pgirmess)#pruebas a posteriori de kruskal wallis
library(MASS)
library(car)
install.packages("colorspace")
GrupoA=c(25,36,36,25,36,16,25,36,49,36,25)
GrupoB=c(121,36,36,64,36,81,49,25,64,49,121)
GrupoC=c(81,81,36,9,25,36,9,49,169,1,81)
GrupoD=c(25,25,36,25,36,25,25,25,25,25,25)
Tiempos=cbind(GrupoA,GrupoB,GrupoC,GrupoD)
tiempo=data.frame("Grupos"=factor(c(rep(1,11),rep(2,11),rep(3,11),rep(4,11))),
"Tiempos"=c(GrupoA,GrupoB,GrupoC,GrupoD))
tiempo
medias= apply(Tiempos,2,mean)
desvios= apply(Tiempos,2,sd)
Resumen5= round(cbind(medias,desvios),3)
plot5= ggplot(Tiempos.aes(x=Grupos,y=Tiempos,fill=Grupos))
install.packages("ggplot2")
install.packages("ggplot")
plot5= ggplot(Tiempos.aes(x=Grupos,y=Tiempos,fill=Grupos))
plot5= ggplot(Tiempos.aes(x=Grupos,y=Tiempos,fill=Grupos))+
geom_boxplot()
library(ggplot)
library(ggplot2) ##Paquete para confeccionar dibujos
plot5= ggplot(Tiempos.aes(x=Grupos,y=Tiempos,fill=Grupos))+
geom_boxplot()
GrupoA=c(25,36,36,25,36,16,25,36,49,36,25)
GrupoB=c(121,36,36,64,36,81,49,25,64,49,121)
GrupoC=c(81,81,36,9,25,36,9,49,169,1,81)
GrupoD=c(25,25,36,25,36,25,25,25,25,25,25)
Tiempos=cbind(GrupoA,GrupoB,GrupoC,GrupoD)
tiempo=data.frame("Grupos"=factor(c(rep(1,11),rep(2,11),rep(3,11),rep(4,11))),
"Tiempos"=c(GrupoA,GrupoB,GrupoC,GrupoD))
tiempo
medias= apply(Tiempos,2,mean)
desvios= apply(Tiempos,2,sd)
Resumen5= round(cbind(medias,desvios),3)
plot5= ggplot(Tiempos.aes(x=Grupos,y=Tiempos,fill=Grupos))+
geom_boxplot()
plot5= ggplot(tiempo.aes(x=Grupos,y=Tiempos,fill=Grupos))+
geom_boxplot()
plot5= ggplot(tiempo,aes(x=Grupos,y=Tiempos,fill=Grupos))+
geom_boxplot()
plot5
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1', echo=TRUE)
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1', echo=TRUE)
proc.time()-t
t <- proc.time() # Inicia el cronómetro
proc.time()-t    # Detiene el cronómetro
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1', echo=TRUE)
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1', echo=TRUE)
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("ranger")
require("randomForest")  #solo se usa para imputar nulos
install.packages("randomforest")
#Aqui se debe poner la carpeta de la computadora local
setwd("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin")
#setwd("D:\\gdrive\\UTN2022P\\")   #Establezco el Working Directory
#cargo los datos donde entreno
dtrain  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#imputo los nulos, ya que ranger no acepta nulos
#Leo Breiman, Â¿por que le temias a los nulos?
dtrain  <- na.roughfix( dtrain )
#cargo los datos donde aplico el modelo
dapply  <- fread("./datasets/paquete_premium_202101.csv", stringsAsFactors= TRUE)
dapply[ , clase_ternaria := NULL ]  #Elimino esta columna que esta toda en NA
dapply  <- na.roughfix( dapply )  #tambien imputo los nulos en los datos donde voy a aplicar el modelo
#genero el modelo de Random Forest con la libreria ranger
#notar como la suma de muchos arboles contrarresta el efecto de min.node.size=1
param  <- list( "num.trees"=       100,  #cantidad de arboles
"mtry"=             4,  #cantidad de variables que evalua para hacer un split  sqrt(ncol(dtrain))
"min.node.size"=  1000,  #tamaÃ±o minimo de las hojas
"max.depth"=        6   # 0 significa profundidad infinita
)
set.seed(999931) #Establezco la semilla aleatoria
#para preparar la posibilidad de asignar pesos a las clases
#la teoria de  Maite San Martin
setorder( dtrain, clase_ternaria )  #primero quedan los BAJA+1, BAJA+2, CONTINUA
#genero el modelo de Random Forest llamando a ranger()
modelo  <- ranger( formula= "clase_ternaria ~ .",
data=  dtrain,
probability=   TRUE,  #para que devuelva las probabilidades
num.trees=     param$num.trees,
mtry=          param$mtry,
min.node.size= param$min.node.size,
max.depth=     param$max.depth
#,class.weights= c( 1,60, 1)  #siguiendo con la idea de Maite San Martin
)
#aplico el modelo recien creado a los datos del futuro
prediccion  <- predict( modelo, dapply )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(prediccion$predictions[ ,"BAJA+2" ] > 1/60) ) ) #genero la salida
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2411/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2411/KA_411_001.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep="," )
f <- proc.time()-t    # Detiene el cronómetro
f
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1', echo=TRUE)
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1', echo=TRUE)
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1')
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1')
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1')
#Se utiliza el algoritmo Random Forest, creado por Leo Breiman en el aÃ±o 2001
#Una libreria que implementa Rando Forest se llama  ranger
#La libreria esta implementada en lenguaje C y corre en paralelo, utiliza TODOS los nucleos del procesador
#Leo Breiman provenia de la estadistica y tenia "horror a los nulos", con lo cual el algoritmo necesita imputar nulos antes
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
# require("data.table")
# require("ranger")
# require("randomForest")  #solo se usa para imputar nulos
# install.packages("randomforest")
#Aqui se debe poner la carpeta de la computadora local
setwd("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin")
#setwd("D:\\gdrive\\UTN2022P\\")   #Establezco el Working Directory
#cargo los datos donde entreno
dtrain  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#imputo los nulos, ya que ranger no acepta nulos
#Leo Breiman, Â¿por que le temias a los nulos?
dtrain  <- na.roughfix( dtrain )
#cargo los datos donde aplico el modelo
dapply  <- fread("./datasets/paquete_premium_202101.csv", stringsAsFactors= TRUE)
dapply[ , clase_ternaria := NULL ]  #Elimino esta columna que esta toda en NA
dapply  <- na.roughfix( dapply )  #tambien imputo los nulos en los datos donde voy a aplicar el modelo
#genero el modelo de Random Forest con la libreria ranger
#notar como la suma de muchos arboles contrarresta el efecto de min.node.size=1
param  <- list( "num.trees"=       100,  #cantidad de arboles
"mtry"=             4,  #cantidad de variables que evalua para hacer un split  sqrt(ncol(dtrain))
"min.node.size"=  1000,  #tamaÃ±o minimo de las hojas
"max.depth"=        6   # 0 significa profundidad infinita
)
set.seed(999931) #Establezco la semilla aleatoria
#para preparar la posibilidad de asignar pesos a las clases
#la teoria de  Maite San Martin
setorder( dtrain, clase_ternaria )  #primero quedan los BAJA+1, BAJA+2, CONTINUA
#genero el modelo de Random Forest llamando a ranger()
modelo  <- ranger( formula= "clase_ternaria ~ .",
data=  dtrain,
probability=   TRUE,  #para que devuelva las probabilidades
num.trees=     param$num.trees,
mtry=          param$mtry,
min.node.size= param$min.node.size,
max.depth=     param$max.depth
#,class.weights= c( 1,60, 1)  #siguiendo con la idea de Maite San Martin
)
#aplico el modelo recien creado a los datos del futuro
prediccion  <- predict( modelo, dapply )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(prediccion$predictions[ ,"BAJA+2" ] > 1/60) ) ) #genero la salida
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2411/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2411/KA_411_001.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep="," )
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1')
#Se utiliza el algoritmo Random Forest, creado por Leo Breiman en el aÃ±o 2001
#Una libreria que implementa Rando Forest se llama  ranger
#La libreria esta implementada en lenguaje C y corre en paralelo, utiliza TODOS los nucleos del procesador
#Leo Breiman provenia de la estadistica y tenia "horror a los nulos", con lo cual el algoritmo necesita imputar nulos antes
start.time <- Sys.time()
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
# require("data.table")
# require("ranger")
# require("randomForest")  #solo se usa para imputar nulos
# install.packages("randomforest")
#Aqui se debe poner la carpeta de la computadora local
setwd("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin")
#setwd("D:\\gdrive\\UTN2022P\\")   #Establezco el Working Directory
#cargo los datos donde entreno
dtrain  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#imputo los nulos, ya que ranger no acepta nulos
#Leo Breiman, Â¿por que le temias a los nulos?
dtrain  <- na.roughfix( dtrain )
#cargo los datos donde aplico el modelo
dapply  <- fread("./datasets/paquete_premium_202101.csv", stringsAsFactors= TRUE)
dapply[ , clase_ternaria := NULL ]  #Elimino esta columna que esta toda en NA
dapply  <- na.roughfix( dapply )  #tambien imputo los nulos en los datos donde voy a aplicar el modelo
#genero el modelo de Random Forest con la libreria ranger
#notar como la suma de muchos arboles contrarresta el efecto de min.node.size=1
param  <- list( "num.trees"=       100,  #cantidad de arboles
"mtry"=             4,  #cantidad de variables que evalua para hacer un split  sqrt(ncol(dtrain))
"min.node.size"=  1000,  #tamaÃ±o minimo de las hojas
"max.depth"=        6   # 0 significa profundidad infinita
)
set.seed(999931) #Establezco la semilla aleatoria
#para preparar la posibilidad de asignar pesos a las clases
#la teoria de  Maite San Martin
setorder( dtrain, clase_ternaria )  #primero quedan los BAJA+1, BAJA+2, CONTINUA
#genero el modelo de Random Forest llamando a ranger()
modelo  <- ranger( formula= "clase_ternaria ~ .",
data=  dtrain,
probability=   TRUE,  #para que devuelva las probabilidades
num.trees=     param$num.trees,
mtry=          param$mtry,
min.node.size= param$min.node.size,
max.depth=     param$max.depth
#,class.weights= c( 1,60, 1)  #siguiendo con la idea de Maite San Martin
)
#aplico el modelo recien creado a los datos del futuro
prediccion  <- predict( modelo, dapply )
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(prediccion$predictions[ ,"BAJA+2" ] > 1/60) ) ) #genero la salida
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2411/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2411/KA_411_001.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep="," )
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger.r", encoding = 'ISO8859-1')
#Se utiliza el algoritmo Random Forest, creado por Leo Breiman en el aÃ±o 2001
#Una libreria que implementa Rando Forest se llama  ranger
#La libreria esta implementada en lenguaje C y corre en paralelo, utiliza TODOS los nucleos del procesador
#Leo Breiman provenia de la estadistica y tenia "horror a los nulos", con lo cual el algoritmo necesita imputar nulos antes
start.time <- Sys.time()
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
# require("data.table")
# require("ranger")
# require("randomForest")  #solo se usa para imputar nulos
# install.packages("randomforest")
#Aqui se debe poner la carpeta de la computadora local
setwd("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin")
#setwd("D:\\gdrive\\UTN2022P\\")   #Establezco el Working Directory
#cargo los datos donde entreno
dtrain  <- fread("./datasets/paquete_premium_202011.csv", stringsAsFactors= TRUE)
#imputo los nulos, ya que ranger no acepta nulos
#Leo Breiman, Â¿por que le temias a los nulos?
dtrain  <- na.roughfix( dtrain )
#cargo los datos donde aplico el modelo
dapply  <- fread("./datasets/paquete_premium_202101.csv", stringsAsFactors= TRUE)
dapply[ , clase_ternaria := NULL ]  #Elimino esta columna que esta toda en NA
dapply  <- na.roughfix( dapply )  #tambien imputo los nulos en los datos donde voy a aplicar el modelo
#genero el modelo de Random Forest con la libreria ranger
#notar como la suma de muchos arboles contrarresta el efecto de min.node.size=1
param  <- list( "num.trees"=       100,  #cantidad de arboles
"mtry"=             4,  #cantidad de variables que evalua para hacer un split  sqrt(ncol(dtrain))
"min.node.size"=  1000,  #tamaÃ±o minimo de las hojas
"max.depth"=        6   # 0 significa profundidad infinita
)
set.seed(999931) #Establezco la semilla aleatoria
#para preparar la posibilidad de asignar pesos a las clases
#la teoria de  Maite San Martin
setorder( dtrain, clase_ternaria )  #primero quedan los BAJA+1, BAJA+2, CONTINUA
#genero el modelo de Random Forest llamando a ranger()
modelo  <- ranger( formula= "clase_ternaria ~ .",
data=  dtrain,
probability=   TRUE,  #para que devuelva las probabilidades
num.trees=     param$num.trees,
mtry=          param$mtry,
min.node.size= param$min.node.size,
max.depth=     param$max.depth
#,class.weights= c( 1,60, 1)  #siguiendo con la idea de Maite San Martin
)
#aplico el modelo recien creado a los datos del futuro
prediccion  <- predict( modelo, dapply )
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
#Genero la entrega para Kaggle
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(prediccion$predictions[ ,"BAJA+2" ] > 1/60) ) ) #genero la salida
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./labo/exp/",  showWarnings = FALSE )
dir.create( "./labo/exp/KA2411/", showWarnings = FALSE )
archivo_salida  <- "./labo/exp/KA2411/KA_411_001.csv"
#genero el archivo para Kaggle
fwrite( entrega,
file= archivo_salida,
sep="," )
time.taken
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/433_ranger_BO.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/lightgbm/552_lightgbm_motivacional.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/lightgbm/552_lightgbm_motivacional.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/lightgbm/552_lightgbm_motivacional.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/lightgbm/552_lightgbm_motivacional.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/lightgbm/552_lightgbm_motivacional.r")
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/ranger/411_ranger_2.r")
?ranger
source("C:/Users/usuario/Desktop/CURSOS_VARIOS/2_Maestría_Datos_UTN/2 AÑO/2-AMD-EcoFin/labo/src/lightgbm/552_lightgbm_motivacional.r")
