3 	 fread(nom_arch) 
1 	 fread(nom_arch) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
1 	 AgregarMes(dataset) 
2 	 ReportarCampos(dataset) 
1 	 CorregirNA(dataset) 
2 	 ReportarCampos(dataset) 
1 	 TendenciaYmuchomas(dataset, cols = cols_lagueables, ventana = PARAM$tendenciaYmuchomas$ventana[i],      tendencia = PARAM$tendenciaYmuchomas$tendencia[i], minimo = PARAM$tendenciaYmuchomas$minimo[i],      maximo = PARAM$tendenciaYmuchomas$maximo[i], promedio = PARAM$tendenciaYmuchomas$promedio[i],      ratioavg = PARAM$tendenciaYmuchomas$ratioavg[i], ratiomax = PARAM$tendenciaYmuchomas$ratiomax[i]) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
2 	 fhistC(dataset[, get(campo)], vector_desde) 
1 	 Lags(cols_lagueables, PARAM$lags$lag[i], PARAM$lags$delta[i]) 
2 	 ReportarCampos(dataset) 
1 	 fwrite(dataset, paste0(PARAM$files$output), logical01 = TRUE,      sep = ",") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
3 	 fread(nom_arch) 
1 	 fread(nom_arch) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
1 	 set.seed(PARAM$semilla) 
5 	 runif(nrow(dataset)) 
1 	 aplicar_particion(seccion) 
1 	 aplicar_particion(seccion) 
1 	 aplicar_particion(seccion) 
1 	 aplicar_particion(seccion) 
1 	 aplicar_particion(seccion) 
1 	 fwrite(tb_control, file = PARAM$files$output$control, sep = "\t") 
1 	 fwrite(dataset[part_future > 0, setdiff(colnames(dataset), c(psecciones,      PARAM$const$clase)), with = FALSE], file = PARAM$files$output$future_data,      logical01 = TRUE, sep = ",") 
1 	 fwrite(dataset[part_train > 0 | part_validate > 0 | part_test >      0, setdiff(colnames(dataset), c("part_future", "part_train_final")),      with = FALSE], file = PARAM$files$output$train_strategy,      logical01 = TRUE, sep = ",") 
1 	 fwrite(dataset[part_train_final > 0, setdiff(colnames(dataset),      psecciones), with = FALSE], file = PARAM$files$output$train_final,      logical01 = TRUE, sep = ",") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
3 	 fread(nom_arch) 
1 	 set.seed(PARAM$semilla) 
1 	 fread(nom_arch) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
1 	 lgb.Dataset(data = data.matrix(dataset[part_train == 1, campos_buenos,      with = FALSE]), label = dataset[part_train == 1, clase01],      weight = dataset[part_train == 1, ifelse(get(PARAM$const$campo_clase) %in%          PARAM$clase_test_POS, 1.0000001, 1)], free_raw_data = FALSE) 
1 	 particionar(dataset, division = c(1, 1), agrupa = c("part_test",      "foto_mes", "clase_ternaria"), seed = PARAM$semilla, campo = "fold_test") 
2 	 set.seed(seed) 
1 	 lgb.Dataset(data = data.matrix(dataset[part_test == 1 & fold_test ==      1, campos_buenos, with = FALSE]), label = dataset[part_test ==      1 & fold_test == 1, clase01], free_raw_data = FALSE) 
1 	 parametrizar(hiperparametros) 
1 	 configureMlr(show.learner.output = FALSE) 
1 	 makeSingleObjectiveFunction(fn = funcion_optimizar, minimize = PARAM$BO$minimize,      noisy = PARAM$BO$noisy, par.set = makeParamSet(params = apertura$paramSet),      has.simple.signature = PARAM$BO$has.simple.signature) 
1 	 makeMBOControl(save.on.disk.at.time = PARAM$BO$save.on.disk.at.time,      save.file.path = PARAM$files$output$BObin) 
2 	 setMBOControlInfill(control) 
2 	 setMBOControlTermination(control, iters = 10L) 
1 	 setMBOControlTermination(ctrl, iters = PARAM$BO$iterations) 
1 	 setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI()) 
1 	 makeLearner("regr.km", predict.type = "se", covtype = "matern3_2",      control = list(trace = TRUE)) 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
1 	 mbo(obj.fun, learner = surr.km, control = ctrl) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.19694239888282, feature_fraction = 0.298903689744475,      num_leaves = 558L, min_data_in_leaf = 2723L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
12 	 lgb.importance(modelo_train) 
11 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.101150194537477, feature_fraction = 0.54364797862363,      num_leaves = 944L, min_data_in_leaf = 5061L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
12 	 lgb.importance(modelo_train) 
11 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.109115315997624, feature_fraction = 0.677252665029664,      num_leaves = 219L, min_data_in_leaf = 5631L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.219439874800737, feature_fraction = 0.631269203702686,      num_leaves = 353L, min_data_in_leaf = 3023L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.157621125917649, feature_fraction = 0.877927442401415,      num_leaves = 419L, min_data_in_leaf = 1483L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.171037877826602, feature_fraction = 0.47209830184438,      num_leaves = 311L, min_data_in_leaf = 3704L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.191502237080131, feature_fraction = 0.154010101330641,      num_leaves = 151L, min_data_in_leaf = 564L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0663912301760865, feature_fraction = 0.760789069002203,      num_leaves = 727L, min_data_in_leaf = 4945L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
12 	 lgb.importance(modelo_train) 
11 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0529878902470227, feature_fraction = 0.230676010684692,      num_leaves = 992L, min_data_in_leaf = 7942L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.29231876291451, feature_fraction = 0.423570122741512,      num_leaves = 876L, min_data_in_leaf = 6593L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.250807821118506, feature_fraction = 0.810647150408477,      num_leaves = 33L, min_data_in_leaf = 4031L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.277056618221686, feature_fraction = 0.585277161101112,      num_leaves = 589L, min_data_in_leaf = 92L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.137662147222436, feature_fraction = 0.326340661995346,      num_leaves = 476L, min_data_in_leaf = 7196L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0739846953656524, feature_fraction = 0.951967658573994,      num_leaves = 121L, min_data_in_leaf = 2068L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0256584717152873, feature_fraction = 0.193567493767478,      num_leaves = 833L, min_data_in_leaf = 1787L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
12 	 lgb.importance(modelo_train) 
11 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
10 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.239024838924524, feature_fraction = 0.926109886700578,      num_leaves = 663L, min_data_in_leaf = 6186L)) 
11 	 set.seed(param_completo$seed) 
11 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0201277133120155, feature_fraction = 0.201970219520254,      num_leaves = 1024L, min_data_in_leaf = 4587L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0201118606852413, feature_fraction = 0.637116204030922,      num_leaves = 1024L, min_data_in_leaf = 6122L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
14 	 lgb.importance(modelo_train) 
13 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0201144288134413, feature_fraction = 0.996276315668719,      num_leaves = 1013L, min_data_in_leaf = 5389L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0706089765495815, feature_fraction = 0.736151588875591,      num_leaves = 994L, min_data_in_leaf = 7997L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200469620309059, feature_fraction = 0.893760691094699,      num_leaves = 16L, min_data_in_leaf = 6531L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200139413900666, feature_fraction = 0.100674011852812,      num_leaves = 1024L, min_data_in_leaf = 2620L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0930577441772488, feature_fraction = 0.722095901024241,      num_leaves = 1024L, min_data_in_leaf = 5877L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200268747956627, feature_fraction = 0.576459622334672,      num_leaves = 771L, min_data_in_leaf = 5509L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0208828494891214, feature_fraction = 0.924246800539754,      num_leaves = 677L, min_data_in_leaf = 7997L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0436558220293725, feature_fraction = 0.622330288274148,      num_leaves = 1024L, min_data_in_leaf = 7630L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200625627112682, feature_fraction = 0.686177822530069,      num_leaves = 1024L, min_data_in_leaf = 4311L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200193376412625, feature_fraction = 0.749036512593369,      num_leaves = 684L, min_data_in_leaf = 6483L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.184976570830897, feature_fraction = 0.998367169057309,      num_leaves = 736L, min_data_in_leaf = 7116L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0821228547262687, feature_fraction = 0.514920480396258,      num_leaves = 833L, min_data_in_leaf = 780L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0513583896177641, feature_fraction = 0.929285615219152,      num_leaves = 16L, min_data_in_leaf = 4865L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.29944924253842, feature_fraction = 0.970969519791997,      num_leaves = 16L, min_data_in_leaf = 5408L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0540554948837449, feature_fraction = 0.893726842806267,      num_leaves = 751L, min_data_in_leaf = 4657L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0555817958812897, feature_fraction = 0.63379674220716,      num_leaves = 1011L, min_data_in_leaf = 5327L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0585180078403881, feature_fraction = 0.961952523987953,      num_leaves = 166L, min_data_in_leaf = 5912L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200037882703878, feature_fraction = 0.674421141329085,      num_leaves = 944L, min_data_in_leaf = 7463L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0792690928248583, feature_fraction = 0.558763144550933,      num_leaves = 16L, min_data_in_leaf = 5596L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0411336375336872, feature_fraction = 0.243620916250549,      num_leaves = 51L, min_data_in_leaf = 3988L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
14 	 lgb.importance(modelo_train) 
13 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0335061487125563, feature_fraction = 0.688398396082715,      num_leaves = 16L, min_data_in_leaf = 2940L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0504823369857198, feature_fraction = 0.136441219221802,      num_leaves = 47L, min_data_in_leaf = 5006L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0392779829569561, feature_fraction = 0.46339718969583,      num_leaves = 112L, min_data_in_leaf = 3955L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0204135477786945, feature_fraction = 0.233478797079259,      num_leaves = 16L, min_data_in_leaf = 2713L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0658547763035437, feature_fraction = 0.950281412115022,      num_leaves = 754L, min_data_in_leaf = 7998L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.104127361947853, feature_fraction = 0.665610834351758,      num_leaves = 762L, min_data_in_leaf = 6605L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0755367408660938, feature_fraction = 0.999456484877898,      num_leaves = 452L, min_data_in_leaf = 5739L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0419819766074623, feature_fraction = 0.264868628063818,      num_leaves = 16L, min_data_in_leaf = 3548L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0237256912618574, feature_fraction = 0.231681516674486,      num_leaves = 67L, min_data_in_leaf = 4229L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0769788369043124, feature_fraction = 0.221127968736589,      num_leaves = 389L, min_data_in_leaf = 4190L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0517786583590773, feature_fraction = 0.232483089272218,      num_leaves = 100L, min_data_in_leaf = 5127L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0793237829412863, feature_fraction = 0.975188061372793,      num_leaves = 17L, min_data_in_leaf = 4633L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0378910248159959, feature_fraction = 0.127318683116626,      num_leaves = 79L, min_data_in_leaf = 4122L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0432624661911263, feature_fraction = 0.539609601762417,      num_leaves = 54L, min_data_in_leaf = 3971L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0202738551359444, feature_fraction = 0.635320816613428,      num_leaves = 87L, min_data_in_leaf = 6386L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0449595248414405, feature_fraction = 0.806012140702521,      num_leaves = 16L, min_data_in_leaf = 4057L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0347141725139819, feature_fraction = 0.325230991729682,      num_leaves = 55L, min_data_in_leaf = 392L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200329491032018, feature_fraction = 0.550097850340558,      num_leaves = 1020L, min_data_in_leaf = 7843L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
14 	 lgb.importance(modelo_train) 
13 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0598582000582551, feature_fraction = 0.612240312613633,      num_leaves = 142L, min_data_in_leaf = 4246L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0755058307669091, feature_fraction = 0.60892110470983,      num_leaves = 78L, min_data_in_leaf = 4162L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0200345134205873, feature_fraction = 0.108096731332462,      num_leaves = 909L, min_data_in_leaf = 2484L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = 178L, min_data_in_leaf = 7196L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
14 	 lgb.importance(modelo_train) 
13 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,      GLOBAL_iteracion, ".txt"), sep = "\t") 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0201113067476885, feature_fraction = 0.618375961783991,      num_leaves = 199L, min_data_in_leaf = 5422L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0431402961597825, feature_fraction = 0.528834888316828,      num_leaves = 184L, min_data_in_leaf = 7307L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0306779945445057, feature_fraction = 0.576465659728923,      num_leaves = 142L, min_data_in_leaf = 2765L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.025372898261263, feature_fraction = 0.604045495006114,      num_leaves = 171L, min_data_in_leaf = 7853L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0382367167881815, feature_fraction = 0.577019668957376,      num_leaves = 236L, min_data_in_leaf = 7873L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
34 	 runif(n * param.n) 
12 	 (structure(function (x)  {     .doTrace(exp_trace_log())     {         gc()         GLOBAL_iteracion <<- GLOBAL_iteracion + 1         param_completo <- c(param_fijos, x)         param_completo$num_iterations <- ifelse(param_fijos$boosting ==              "dart", 999, 99999)         param_completo$early_stopping_rounds <- as.integer(200 +              4/param_completo$learning_rate)         vprob_optima <<- c()         set.seed(param_completo$seed)         modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),              eval = fganancia_lgbm_meseta, param = param_completo,              verbose = -100)         prob_corte <- vprob_optima[modelo_train$best_iter]         prediccion <- predict(modelo_train, data.matrix(dataset_test[,              campos_buenos, with = FALSE]))         tbl <- dataset_test[, list(clase01)]         tbl[, `:=`(prob, prediccion)]         ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,              PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]         cantidad_test_normalizada <- test_multiplicador * tbl[prob >=              prob_corte, .N]         rm(tbl)         gc()         ganancia_test_normalizada <- test_multiplicador * ganancia_test         if (ganancia_test_normalizada > GLOBAL_ganancia) {             GLOBAL_ganancia <<- ganancia_test_normalizada             tb_importancia <- as.data.table(lgb.importance(modelo_train))             fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,                  GLOBAL_iteracion, ".txt"), sep = "\t")         }         ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))         xx <- c(ds, copy(param_completo))         xx$early_stopping_rounds <- NULL         xx$num_iterations <- modelo_train$best_iter         xx$prob_corte <- prob_corte         xx$estimulos <- cantidad_test_normalizada         xx$ganancia <- ganancia_test_normalizada         xx$iteracion_bayesiana <- GLOBAL_iteracion         exp_log(xx, arch = PARAM$files$output$BOlog)         return(ganancia_test_normalizada)     } }, original = function (x)  {     gc()     GLOBAL_iteracion <<- GLOBAL_iteracion + 1     param_completo <- c(param_fijos, x)     param_completo$num_iterations <- ifelse(param_fijos$boosting ==          "dart", 999, 99999)     param_completo$early_stopping_rounds <- as.integer(200 +          4/param_completo$learning_rate)     vprob_optima <<- c()     set.seed(param_completo$seed)     modelo_train <- lgb.train(data = dtrain, valids = list(valid = dvalidate),          eval = fganancia_lgbm_meseta, param = param_completo,          verbose = -100)     prob_corte <- vprob_optima[modelo_train$best_iter]     prediccion <- predict(modelo_train, data.matrix(dataset_test[,          campos_buenos, with = FALSE]))     tbl <- dataset_test[, list(clase01)]     tbl[, `:=`(prob, prediccion)]     ganancia_test <- tbl[prob >= prob_corte, sum(ifelse(clase01,          PARAM$const$POS_ganancia, PARAM$const$NEG_ganancia))]     cantidad_test_normalizada <- test_multiplicador * tbl[prob >=          prob_corte, .N]     rm(tbl)     gc()     ganancia_test_normalizada <- test_multiplicador * ganancia_test     if (ganancia_test_normalizada > GLOBAL_ganancia) {         GLOBAL_ganancia <<- ganancia_test_normalizada         tb_importancia <- as.data.table(lgb.importance(modelo_train))         fwrite(tb_importancia, file = paste0(PARAM$files$output$importancia,              GLOBAL_iteracion, ".txt"), sep = "\t")     }     ds <- list(cols = ncol(dtrain), rows = nrow(dtrain))     xx <- c(ds, copy(param_completo))     xx$early_stopping_rounds <- NULL     xx$num_iterations <- modelo_train$best_iter     xx$prob_corte <- prob_corte     xx$estimulos <- cantidad_test_normalizada     xx$ganancia <- ganancia_test_normalizada     xx$iteracion_bayesiana <- GLOBAL_iteracion     exp_log(xx, arch = PARAM$files$output$BOlog)     return(ganancia_test_normalizada) }, source = <environment>, class = c("smoof_single_objective_function",  "smoof_function", "function"), name = "", id = NA, description = "", par.set = structure(list(     pars = list(learning_rate = structure(list(id = "learning_rate",          type = "numeric", len = 1L, lower = 0.02, upper = 0.3,          values = NULL, cnames = NULL, allow.inf = FALSE, has.default = FALSE,          default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,          special.vals = list()), class = "Param"), feature_fraction = structure(list(         id = "feature_fraction", type = "numeric", len = 1L,          lower = 0.1, upper = 1, values = NULL, cnames = NULL,          allow.inf = FALSE, has.default = FALSE, default = NULL,          trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param"),          num_leaves = structure(list(id = "num_leaves", type = "integer",              len = 1L, lower = 16, upper = 1024, values = NULL,              cnames = NULL, allow.inf = FALSE, has.default = FALSE,              default = NULL, trafo = NULL, requires = NULL, tunable = TRUE,              special.vals = list()), class = "Param"), min_data_in_leaf = structure(list(             id = "min_data_in_leaf", type = "integer", len = 1L,              lower = 0, upper = 8000, values = NULL, cnames = NULL,              allow.inf = FALSE, has.default = FALSE, default = NULL,              trafo = NULL, requires = NULL, tunable = TRUE, special.vals = list()), class = "Param")),      forbidden = NULL), class = "ParamSet"), noisy = TRUE, minimize = FALSE, vectorized = FALSE, n.objectives = 1L, tags = character(0)))(x = list(     learning_rate = 0.0201164585052117, feature_fraction = 0.588469693904518,      num_leaves = 955L, min_data_in_leaf = 6219L)) 
13 	 set.seed(param_completo$seed) 
13 	 lgb.train(data = dtrain, valids = list(valid = dvalidate), eval = fganancia_lgbm_meseta,      param = param_completo, verbose = -100) 
30 	 runif(n * param.n) 
3 	 fread(nom_arch) 
1 	 set.seed(PARAM$semilla_primos) 
1 	 fread(nom_arch) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
1 	 fread(nom_arch) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
12 	 set.seed(seed = seed, ...) 
1 	 fread(nom_arch) 
1 	 lgb.Dataset(data = data.matrix(dataset[, campos_buenos, with = FALSE]),      label = dataset[, clase01], weight = dataset[, ifelse(get(PARAM$const$campo_clase) %in%          PARAM$clase_test_POS, 1.0000001, 1)], free_raw_data = FALSE) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 999983L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
2 	 lgb.importance(modelo_final) 
1 	 fwrite(tb_importancia, file = paste0(PARAM$files$output$FMimportancia,      sprintf("%03d", iteracion_bayesiana), ".txt"), sep = "\t") 
1 	 fwrite(tb_modelos, file = PARAM$files$output$tb_modelos, sep = "\t") 
1 	 fwrite(tb_prediccion, file = nom_pred, sep = "\t") 
1 	 fwrite(tb_predicciones, file = PARAM$files$output$tb_predicciones,      sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
8 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 707753L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 491417L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 599309L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 563113L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 672767L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 760477L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 910981L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 514201L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 604073L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 773453L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 346417L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 148483L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 856547L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 873979L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 502819L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 388879L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 748723L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 720359L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 222247L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 432727L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 571867L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 283211L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 361903L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 942317L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 474757L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 406873L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 353173L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 613381L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 616027L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 628037L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 778109L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 419753L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 578971L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 190633L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 956953L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 608591L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 245899L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 897077L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 116041L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 289847L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 353737L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 285497L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 223129L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 400643L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 541543L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 461917L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 397151L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 374687L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 444253L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 set.seed(parametros$seed) 
3 	 (new("functionWithTrace", .Data = function (params = list(),      data, nrounds = 100L, valids = list(), obj = NULL, eval = NULL,      verbose = 1L, record = TRUE, eval_freq = 1L, init_model = NULL,      colnames = NULL, categorical_feature = NULL, early_stopping_rounds = NULL,      callbacks = list(), reset_data = FALSE, serializable = TRUE)  {     .doTrace(exp_trace_log())     {         if (nrounds <= 0L) {             stop("nrounds should be greater than zero")         }         if (!lgb.is.Dataset(x = data)) {             stop("lgb.train: data must be an lgb.Dataset instance")         }         if (length(valids) > 0L) {             if (!identical(class(valids), "list") || !all(vapply(valids,                  lgb.is.Dataset, logical(1L)))) {                 stop("lgb.train: valids must be a list of lgb.Dataset elements")             }             evnames <- names(valids)             if (is.null(evnames) || !all(nzchar(evnames))) {                 stop("lgb.train: each element of valids must have a name")             }         }         params <- lgb.check.wrapper_param(main_param_name = "verbosity",              params = params, alternative_kwarg_value = verbose)         params <- lgb.check.wrapper_param(main_param_name = "num_iterations",              params = params, alternative_kwarg_value = nrounds)         params <- lgb.check.wrapper_param(main_param_name = "metric",              params = params, alternative_kwarg_value = NULL)         params <- lgb.check.wrapper_param(main_param_name = "objective",              params = params, alternative_kwarg_value = obj)         params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",              params = params, alternative_kwarg_value = early_stopping_rounds)         early_stopping_rounds <- params[["early_stopping_round"]]         fobj <- NULL         if (is.function(params$objective)) {             fobj <- params$objective             params$objective <- "none"         }         params <- lgb.check.eval(params = params, eval = eval)         eval_functions <- list(NULL)         if (is.function(eval)) {             eval_functions <- list(eval)         }         if (methods::is(eval, "list")) {             eval_functions <- Filter(f = is.function, x = eval)         }         predictor <- NULL         if (is.character(init_model)) {             predictor <- Predictor$new(modelfile = init_model)         }         else if (lgb.is.Booster(x = init_model)) {             predictor <- init_model$to_predictor()         }         begin_iteration <- 1L         if (!is.null(predictor)) {             begin_iteration <- predictor$current_iter() + 1L         }         end_iteration <- begin_iteration + params[["num_iterations"]] -              1L         interaction_constraints <- params[["interaction_constraints"]]         params["interaction_constraints"] <- NULL         data$update_params(params = params)         data$construct()         cnames <- NULL         if (!is.null(colnames)) {             cnames <- colnames         }         else if (!is.null(data$get_colnames())) {             cnames <- data$get_colnames()         }         params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,              column_names = cnames)         data$update_params(params)         data$.__enclos_env__$private$set_predictor(predictor)         if (!is.null(colnames)) {             data$set_colnames(colnames)         }         if (!is.null(categorical_feature)) {             data$set_categorical_feature(categorical_feature)         }         valid_contain_train <- FALSE         train_data_name <- "train"         reduced_valid_sets <- list()         if (length(valids) > 0L) {             for (key in names(valids)) {                 valid_data <- valids[[key]]                 if (identical(data, valid_data)) {                   valid_contain_train <- TRUE                   train_data_name <- key                   next                 }                 valid_data$update_params(params)                 valid_data$set_reference(data)                 reduced_valid_sets[[key]] <- valid_data             }         }         if (params[["verbosity"]] > 0L && eval_freq > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))         }         if (record && length(valids) > 0L) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())         }         using_early_stopping <- !is.null(early_stopping_rounds) &&              early_stopping_rounds > 0L         boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]         using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {             identical(params[[param]], "dart")         }))         if (using_dart) {             warning("Early stopping is not available in 'dart' mode.")             using_early_stopping <- FALSE             callbacks <- Filter(f = function(cb_func) {                 !identical(attr(cb_func, "name"), "cb_early_stop")             }, x = callbacks)         }         if (using_early_stopping) {             callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,                  first_metric_only = isTRUE(params[["first_metric_only"]]),                  verbose = params[["verbosity"]] > 0L))         }         cb <- categorize.callbacks(cb_list = callbacks)         booster <- Booster$new(params = params, train_set = data)         if (valid_contain_train) {             booster$set_train_data_name(name = train_data_name)         }         for (key in names(reduced_valid_sets)) {             booster$add_valid(data = reduced_valid_sets[[key]],                  name = key)         }         env <- CB_ENV$new()         env$model <- booster         env$begin_iteration <- begin_iteration         env$end_iteration <- end_iteration         for (i in seq.int(from = begin_iteration, to = end_iteration)) {             env$iteration <- i             env$eval_list <- list()             for (f in cb$pre_iter) {                 f(env)             }             booster$update(fobj = fobj)             eval_list <- list()             if (length(valids) > 0L) {                 for (eval_function in eval_functions) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }                 if (length(eval_functions) == 0L) {                   if (valid_contain_train) {                     eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                   }                   eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))                 }             }             env$eval_list <- eval_list             for (f in cb$post_iter) {                 f(env)             }             if (env$met_early_stop)                  break         }         non_train_valid_names <- names(valids)[!(names(valids) ==              train_data_name)]         first_valid_name <- non_train_valid_names[1L]         if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {             if (!is.null(eval_functions[1L])) {                 first_metric <- names(booster$record_evals[[first_valid_name]])[1L]             }             else {                 first_metric <- booster$.__enclos_env__$private$eval_names[1L]             }             .find_best <- which.min             if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {                 .find_best <- which.max             }             booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))             booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]         }         if (reset_data) {             booster_old <- list(best_iter = booster$best_iter,                  best_score = booster$best_score, record_evals = booster$record_evals)             booster <- lgb.load(model_str = booster$save_model_to_string())             booster$best_iter <- booster_old$best_iter             booster$best_score <- booster_old$best_score             booster$record_evals <- booster_old$record_evals         }         if (serializable) {             booster$save_raw()         }         return(booster)     } }, original = function (params = list(), data, nrounds = 100L,      valids = list(), obj = NULL, eval = NULL, verbose = 1L, record = TRUE,      eval_freq = 1L, init_model = NULL, colnames = NULL, categorical_feature = NULL,      early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,      serializable = TRUE)  {     if (nrounds <= 0L) {         stop("nrounds should be greater than zero")     }     if (!lgb.is.Dataset(x = data)) {         stop("lgb.train: data must be an lgb.Dataset instance")     }     if (length(valids) > 0L) {         if (!identical(class(valids), "list") || !all(vapply(valids,              lgb.is.Dataset, logical(1L)))) {             stop("lgb.train: valids must be a list of lgb.Dataset elements")         }         evnames <- names(valids)         if (is.null(evnames) || !all(nzchar(evnames))) {             stop("lgb.train: each element of valids must have a name")         }     }     params <- lgb.check.wrapper_param(main_param_name = "verbosity",          params = params, alternative_kwarg_value = verbose)     params <- lgb.check.wrapper_param(main_param_name = "num_iterations",          params = params, alternative_kwarg_value = nrounds)     params <- lgb.check.wrapper_param(main_param_name = "metric",          params = params, alternative_kwarg_value = NULL)     params <- lgb.check.wrapper_param(main_param_name = "objective",          params = params, alternative_kwarg_value = obj)     params <- lgb.check.wrapper_param(main_param_name = "early_stopping_round",          params = params, alternative_kwarg_value = early_stopping_rounds)     early_stopping_rounds <- params[["early_stopping_round"]]     fobj <- NULL     if (is.function(params$objective)) {         fobj <- params$objective         params$objective <- "none"     }     params <- lgb.check.eval(params = params, eval = eval)     eval_functions <- list(NULL)     if (is.function(eval)) {         eval_functions <- list(eval)     }     if (methods::is(eval, "list")) {         eval_functions <- Filter(f = is.function, x = eval)     }     predictor <- NULL     if (is.character(init_model)) {         predictor <- Predictor$new(modelfile = init_model)     }     else if (lgb.is.Booster(x = init_model)) {         predictor <- init_model$to_predictor()     }     begin_iteration <- 1L     if (!is.null(predictor)) {         begin_iteration <- predictor$current_iter() + 1L     }     end_iteration <- begin_iteration + params[["num_iterations"]] -          1L     interaction_constraints <- params[["interaction_constraints"]]     params["interaction_constraints"] <- NULL     data$update_params(params = params)     data$construct()     cnames <- NULL     if (!is.null(colnames)) {         cnames <- colnames     }     else if (!is.null(data$get_colnames())) {         cnames <- data$get_colnames()     }     params[["interaction_constraints"]] <- lgb.check_interaction_constraints(interaction_constraints = interaction_constraints,          column_names = cnames)     data$update_params(params)     data$.__enclos_env__$private$set_predictor(predictor)     if (!is.null(colnames)) {         data$set_colnames(colnames)     }     if (!is.null(categorical_feature)) {         data$set_categorical_feature(categorical_feature)     }     valid_contain_train <- FALSE     train_data_name <- "train"     reduced_valid_sets <- list()     if (length(valids) > 0L) {         for (key in names(valids)) {             valid_data <- valids[[key]]             if (identical(data, valid_data)) {                 valid_contain_train <- TRUE                 train_data_name <- key                 next             }             valid_data$update_params(params)             valid_data$set_reference(data)             reduced_valid_sets[[key]] <- valid_data         }     }     if (params[["verbosity"]] > 0L && eval_freq > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_print_evaluation(period = eval_freq))     }     if (record && length(valids) > 0L) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_record_evaluation())     }     using_early_stopping <- !is.null(early_stopping_rounds) &&          early_stopping_rounds > 0L     boosting_param_names <- .PARAMETER_ALIASES()[["boosting"]]     using_dart <- any(sapply(X = boosting_param_names, FUN = function(param) {         identical(params[[param]], "dart")     }))     if (using_dart) {         warning("Early stopping is not available in 'dart' mode.")         using_early_stopping <- FALSE         callbacks <- Filter(f = function(cb_func) {             !identical(attr(cb_func, "name"), "cb_early_stop")         }, x = callbacks)     }     if (using_early_stopping) {         callbacks <- add.cb(cb_list = callbacks, cb = cb_early_stop(stopping_rounds = early_stopping_rounds,              first_metric_only = isTRUE(params[["first_metric_only"]]),              verbose = params[["verbosity"]] > 0L))     }     cb <- categorize.callbacks(cb_list = callbacks)     booster <- Booster$new(params = params, train_set = data)     if (valid_contain_train) {         booster$set_train_data_name(name = train_data_name)     }     for (key in names(reduced_valid_sets)) {         booster$add_valid(data = reduced_valid_sets[[key]], name = key)     }     env <- CB_ENV$new()     env$model <- booster     env$begin_iteration <- begin_iteration     env$end_iteration <- end_iteration     for (i in seq.int(from = begin_iteration, to = end_iteration)) {         env$iteration <- i         env$eval_list <- list()         for (f in cb$pre_iter) {             f(env)         }         booster$update(fobj = fobj)         eval_list <- list()         if (length(valids) > 0L) {             for (eval_function in eval_functions) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }             if (length(eval_functions) == 0L) {                 if (valid_contain_train) {                   eval_list <- append(eval_list, booster$eval_train(feval = eval_function))                 }                 eval_list <- append(eval_list, booster$eval_valid(feval = eval_function))             }         }         env$eval_list <- eval_list         for (f in cb$post_iter) {             f(env)         }         if (env$met_early_stop)              break     }     non_train_valid_names <- names(valids)[!(names(valids) ==          train_data_name)]     first_valid_name <- non_train_valid_names[1L]     if (record && length(non_train_valid_names) > 0L && is.na(env$best_score)) {         if (!is.null(eval_functions[1L])) {             first_metric <- names(booster$record_evals[[first_valid_name]])[1L]         }         else {             first_metric <- booster$.__enclos_env__$private$eval_names[1L]         }         .find_best <- which.min         if (isTRUE(env$eval_list[[1L]]$higher_better[1L])) {             .find_best <- which.max         }         booster$best_iter <- unname(.find_best(unlist(booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]])))         booster$best_score <- booster$record_evals[[first_valid_name]][[first_metric]][[.EVAL_KEY()]][[booster$best_iter]]     }     if (reset_data) {         booster_old <- list(best_iter = booster$best_iter, best_score = booster$best_score,              record_evals = booster$record_evals)         booster <- lgb.load(model_str = booster$save_model_to_string())         booster$best_iter <- booster_old$best_iter         booster$best_score <- booster_old$best_score         booster$record_evals <- booster_old$record_evals     }     if (serializable) {         booster$save_raw()     }     return(booster) }, source = <environment>))(params = list(lambda_l1 = 0L, lambda_l2 = 0L,      min_gain_to_split = 0L, bagging_fraction = 1L, pos_bagging_fraction = 1L,      neg_bagging_fraction = 1L, max_depth = -1L, max_bin = 31L,      seed = 584971L, extra_trees = FALSE, drop_rate = 0.1, max_drop = 50L,      skip_drop = 0.5, metric = "custom", first_metric_only = TRUE,      objective = "binary", boost_from_average = TRUE, force_row_wise = TRUE,      feature_pre_filter = FALSE, boosting = "gbdt", num_threads = 0L,      verbosity = -100L, learning_rate = 0.030968429811825, feature_fraction = 0.570969160784186,      num_leaves = integer(0), min_data_in_leaf = 7196L, num_iterations = 296L),      data = <environment>, nrounds = 100L, obj = "regression",      verbose = -100L, eval_freq = 1L, early_stopping_rounds = NULL,      init_model = NULL, callbacks = list(), serializable = TRUE,      valids = list()) 
4 	 stats::runif(n) 
1 	 fwrite(tb_prediccion_semillerio, file = nom_pred_semillerio,      sep = "\t") 
1 	 fwrite(tb_predicciones, file = PARAM$files$output$tb_predicciones,      sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
1 	 fwrite(tb_prediccion_semillerio[, c(PARAM$const$campo_id, PARAM$const$campo_pred),      with = FALSE], file = nom_submit, sep = ",") 
1 	 fwrite(tb_submits, file = PARAM$files$output$tb_submits, sep = "\t") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
2 	 fwrite(tb_catalogo, file = EXP$environment$catalog, sep = "\t") 
